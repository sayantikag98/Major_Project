# -*- coding: utf-8 -*-
"""Chatter_time_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1reoIC8-faGnTL4oqQU3QcvtL5wuY9JbU

# MOUNTING GOOGLE DRIVE
"""

from google.colab import drive
drive.mount("drive", force_remount = False)

"""# IMPORTING PYTHON LIBRARIES"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import scipy
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.feature_selection import f_regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import r2_score
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_squared_log_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import BayesianRidge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNetCV
from sklearn.linear_model import SGDRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor
import xgboost 
from sklearn.svm import SVR
import keras
from keras.models import Sequential
from keras.layers import Dense
from sklearn.feature_selection import SelectKBest, f_regression
from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
warnings.simplefilter("ignore")

"""# DATA READING"""

path = "drive/MyDrive/feature_data_regression.csv"
df = pd.read_csv(path)

"""# DATA EXPLORATION"""

df.info()

df.columns

df.describe()

df.isna().sum()

"""# FEATURE SCALING - NORMALIZATION"""

minmax = MinMaxScaler(feature_range = (0,1))
for x in df.columns:
  if (x != "stickout_distance_inches" and x != "cutting_speed_rpm" and x != "time_of_onset_of_chatter_in_sec"):
    df[x] = minmax.fit_transform(np.array(df[x]).reshape(-1,1))

df

"""# DROPPING COLUMNS WITH SAME VALUE"""

df.drop(df.std()[(df.std() == 0)].index, axis=1, inplace = True)

df1 = df.iloc[:, 3:-1]
df1

"""# LINE PLOT"""

# df1.plot(subplots = True, linewidth = 2.0, figsize = (30,500))
# plt.show()

"""# BOX PLOT"""

# df1.boxplot(figsize = (100, 100), grid = True)
# plt.show()

"""# DENSITY PLOT"""

# df1.plot(subplots=True,kind='density',figsize=(100,100))
# plt.show()

"""# PAIR PLOT"""

# plt.figure(figsize = (400, 600))
  # sns.pairplot(df1,diag_kind='kde')
  # plt.show()

"""# HEAT MAP"""

# plt.figure(figsize = (400, 600))
# corr = df1.corr()
# sns.heatmap(corr, annot = True, cmap = "tab20")
# plt.show()

df.shape

"""# ONE HOT ENCODING"""

sd = pd.get_dummies(df['stickout_distance_inches'], prefix = "stickout")

cs = pd.get_dummies(df.cutting_speed_rpm, prefix = "cutting_speed")

df = pd.concat([sd,cs, df.iloc[:,2:]], axis = 1)

df

"""# FEATURE SELECTION USING VARIANCE INFLATION FACTOR"""

def calc_vif(X):
    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return(vif)

vif_res = calc_vif(df.iloc[:,10:-1])

col_name = vif_res[vif_res.VIF>10].iloc[:,0].tolist()

col_name

df.shape

df.drop(columns = col_name, inplace = True, axis = 1)

df.to_csv("Prediction_data.csv", index = False, encoding = "utf-8")

"""# FEATURE TARGET SPLIT"""

X = df.iloc[:,:-1].values

X.shape

y = df.iloc[:,-1].values

y.shape

"""# TRAIN TEST VAL SPLIT"""

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 101)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size = 0.8, random_state = 101)

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)
print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

"""# REGRESSION METRIC"""

def regression_metric(X_val, y_val, y_pred):
  for i in range(len(y_pred)):
    if (y_pred[i]<0):
      y_pred[i] = -1
  explained_Variance_Score = explained_variance_score(y_val, y_pred)
  mean_Absolute_Error = mean_absolute_error(y_val,y_pred)
  median_Absolute_Error = median_absolute_error(y_val, y_pred)
  mean_Squared_Error = mean_squared_error(y_val,y_pred)
  root_Mean_Squared_Error = mean_Squared_Error**(1/2)
  r2_Score = r2_score(y_val, y_pred)
  ### to calculate the adjusted r^2 score from the formula given in theory 
  adjusted_r_squared = 1 - (1-r2_Score)*(len(y_val)-1)/(len(y_val)-X_val.shape[1]-1)
  return explained_Variance_Score, mean_Absolute_Error, median_Absolute_Error, mean_Squared_Error, root_Mean_Squared_Error, r2_Score, adjusted_r_squared

"""# REGRESSOR"""

def regressor(X_train, x_val, x_test, y_train, y_val, y_test, obj):
  cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
  scores = cross_val_score(obj, X_train, y_train, scoring='r2', cv=cv)
  scores_mean = scores.mean()
  scores_std = scores.std()
  y_pred=obj.predict(X_val)
  score_train = obj.score(X_train,y_train)
  score_val = obj.score(X_val,y_val)
  score_test = obj.score(X_test, y_test)
  explained_Variance_Score, mean_Absolute_Error, median_Absolute_Error, mean_Squared_Error, root_Mean_Squared_Error, r2_Score, adjusted_r_squared = regression_metric(X_val, y_val, y_pred)
  y_pred_ans = obj.predict(X_test)
  df_1=pd.DataFrame({'Predicted_values_Answer': y_pred_ans})
  lis = [explained_Variance_Score, mean_Absolute_Error, median_Absolute_Error, mean_Squared_Error, root_Mean_Squared_Error, r2_Score, adjusted_r_squared,scores_mean, scores_std, score_train, score_val, score_test]
  return df_1, lis

"""# LINEAR REGRESSION"""

reg = LinearRegression().fit(X_train, y_train)
df_reg , lis_reg = regressor(X_train, X_val, X_test, y_train, y_val, y_test, reg)

"""# RIDGE REGRESSION"""

rid = Ridge().fit(X_train,y_train)
df_rid, lis_rid = regressor(X_train, X_val, X_test, y_train, y_val, y_test, rid)

"""# BAYESIAN RIDGE REGRESSION"""

bay = BayesianRidge().fit(X_train,y_train)
df_bay, lis_bay = regressor(X_train, X_val, X_test, y_train, y_val, y_test, bay)

"""# LASSO REGRESSION"""

las = Lasso().fit(X_train,y_train)
df_las, lis_las = regressor(X_train, X_val, X_test, y_train, y_val, y_test, las)

"""# ELASTIC NET CV"""

ela = ElasticNetCV().fit(X_train,y_train)
df_ela, lis_ela = regressor(X_train, X_val, X_test, y_train, y_val, y_test, ela)

"""# SGD REGRESSOR"""

sgd = SGDRegressor().fit(X_train,y_train)
df_sgd, lis_sgd = regressor(X_train, X_val, X_test, y_train, y_val, y_test, sgd)

"""# DECISION TREE REGRESSOR"""

dec = DecisionTreeRegressor().fit(X_train,y_train)
df_dec, lis_dec = regressor(X_train, X_val, X_test, y_train, y_val, y_test, dec)

"""# ADA BOOST REGRESSOR"""

ada = AdaBoostRegressor(DecisionTreeRegressor()).fit(X_train,y_train)
df_ada, lis_ada = regressor(X_train, X_val, X_test, y_train, y_val, y_test, ada)

"""# RANDOM FOREST REGRESSOR"""

ran = RandomForestRegressor().fit(X_train,y_train)
df_ran, lis_ran = regressor(X_train, X_val, X_test, y_train, y_val, y_test, ran)

"""# EXTRA TREE REGRESSOR"""

ext = ExtraTreesRegressor().fit(X_train,y_train)
df_ext, lis_ext = regressor(X_train, X_val, X_test, y_train, y_val, y_test, ext)

"""# GRADIENT BOOSTING REGRESSOR"""

gra = GradientBoostingRegressor().fit(X_train,y_train)
df_gra, lis_gra = regressor(X_train, X_val, X_test, y_train, y_val, y_test, gra)

"""# XG BOOST REGRESSOR"""

eval_set = [(X_train, y_train), (X_val, y_val)]
xgb = xgboost.XGBRegressor(objective='reg:squarederror',random_state=42,learning_rate = 0.1, n_estimators= 100, max_depth= 3, subsample= 0.8, colsample_bytree= 1, gamma= 1).fit(X_train,y_train, eval_metric="rmse", eval_set=eval_set, verbose=False, early_stopping_rounds=42)
results_1 = xgb.evals_result()
print(results_1)
epochs = len(results_1['validation_0']['rmse'])
x_axis = range(0, epochs)
# plotting rmse error
fig, ax = plt.subplots()
ax.plot(x_axis, results_1['validation_0']['rmse'], label='Train')
ax.plot(x_axis, results_1['validation_1']['rmse'], label='Test')
ax.legend()
plt.ylabel('RMS Error')
plt.title('XGBoost Regression Error')
plt.show()
df_xgb, lis_xgb = regressor(X_train, X_val, X_test, y_train, y_val, y_test, xgb)

"""#  SVM SVR"""

svr = SVR(C=100,kernel='rbf').fit(X_train,y_train)
df_svr, lis_svr = regressor(X_train, X_val, X_test, y_train, y_val, y_test, svr)

reg_list = ["metric_name", "linear_regression", "rigde_regression", "bayesian_ridge_regression", "lasso_regression", "elastic_net_cv", "stochastic_gradient_descent", "decision_tree", "ada_boost", "random_forest", "extra_tree", "gradient_boosting", "xg_boost", "svm_svr"]

df_res = pd.DataFrame(['explained_Variance_Score', 'mean_Absolute_Error', 'median_Absolute_Error', 'mean_Squared_Error', 'root_Mean_Squared_Error', 'r2_Score', 'adjusted_r_squared', 'cross_validation_mean_score_train', 'cross_validation_std_score_train', 'cross_validation_train_score', 'cross_validation_val_score', 'cross_validation_test_score'], index=None)
df_res = pd.concat([df_res, pd.DataFrame(lis_reg), pd.DataFrame(lis_rid), pd.DataFrame(lis_bay), pd.DataFrame(lis_las), pd.DataFrame(lis_ela), pd.DataFrame(lis_sgd), pd.DataFrame(lis_dec), pd.DataFrame(lis_ada), pd.DataFrame(lis_ran), pd.DataFrame(lis_ext), pd.DataFrame(lis_gra), pd.DataFrame(lis_xgb), pd.DataFrame(lis_svr)], axis = 1)

df_res.columns = reg_list
df_res.reset_index(drop = True, inplace = True)

df_res = df_res.transpose()
df_res

col_name = df_res.iloc[0,:].tolist()

col_name

df_res.columns = col_name

df_res = df_res.iloc[1:,:]

df_res.sort_values(by = "adjusted_r_squared", inplace = True, ascending = False)

df_res

df_res.to_csv("result_chatter_prediction.csv", encoding = "utf-8")

y_pred_test = df_ext.iloc[:,0].tolist()
for i in range(len(y_pred_test)):
  if (y_pred_test[i] < 0):
    y_pred_test[i] = -1

true_value = df_ans.iloc[:,0]
predicted_value = df_ans.iloc[:,1]
plt.figure(figsize = (10,10))
plt.scatter(true_value, predicted_value, c='crimson', s = 2)

p1 = max(max(predicted_value), max(true_value))
p2 = min(min(predicted_value), min(true_value))
plt.plot([p1, p2], [p1, p2], 'b-')
plt.xlabel('Actual Onset Time in seconds', fontsize=15)
plt.ylabel('Predicted Onset Time in seconds', fontsize=15)
plt.text(5, 30, 'Late Prediction', style='italic', fontsize = 12, bbox={
        'facecolor': 'yellow', 'alpha': 0.5, 'pad': 10})
plt.text(30, 10, 'Early Prediction', style='italic', fontsize = 12, bbox={
        'facecolor': 'yellow', 'alpha': 0.5, 'pad': 10})
plt.annotate('Same Prediction', xy=(30, 30), xytext=(35, 25), fontsize=12,
            arrowprops=dict(facecolor='green', shrink=0.05))
plt.savefig("chatter_prediction.png")
plt.show()

count = 0
for i in range(len(df_ans)):
  if (df_ans.iloc[i,0] >= df_ans.iloc[i,1]):
    count += 1

early_or_same_prediction = count/len(df_ans)

early_or_same_prediction

late_prediction = (len(df_ans) - count)/len(df_ans)

late_prediction

