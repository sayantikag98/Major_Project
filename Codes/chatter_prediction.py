# -*- coding: utf-8 -*-
"""Chatter_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1leA8wFTGoS64VybAVWQfPnkR6ZUojWcv
"""

from google.colab import drive 
drive.mount('drive', force_remount = False)

pip install ordpy

pip install statistics

pip install tsfel

import pandas as pd
import numpy as np
import time
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn
from scipy.io import loadmat
from sklearn.model_selection import train_test_split
import scipy
from sklearn import preprocessing
import random
import ordpy
from scipy import signal
import statistics
import math
import tsfel

font = {
  'weight':'bold',
  'size' : 22 
}
plt.rc('font', **font)

def create_csv(path):
  data = loadmat(path)
  time_list = []
  for i in range(len(data['t'])):
    for j in data['t'][i]:
      time_list.append(j)
  acc_list = []
  for i in range(len(data['d'])):
    acc_list.append(data['d'][i][4])
  df = pd.DataFrame({'time':time_list,'acc_x':acc_list})
  return df

def resample_acc_x (acc_list):
  i = 0
  j = 16
  avg_acc_list = []
  while(j<len(acc_list)):
    sum = 0
    count = 0
    for k in range(i,j):
      sum += acc_list[k]
      count += 1
    avg = sum/count
    avg_acc_list.append(avg)
    i += 16
    j += 16
  sum = 0
  count = 0
  for k in range(i, len(acc_list)):
    sum += acc_list[k];
    count += 1
  
  avg = sum/count
  avg_acc_list.append(avg)
  time_list = []
  i = 0
  count = 0
  while(count<len(avg_acc_list)):
    time_list.append(i)
    i += (1/10000)
    count += 1
  df = pd.DataFrame({'time':time_list, 'acc_x':avg_acc_list})
  return df

def data_func(path):
  df = create_csv(path)
  acc_x_list = df.acc_x.tolist()

  ######## for filtering the data #################################

  sos = signal.butter(100, 1/16, 'low', analog = False, output = 'sos')
  filtered = signal.sosfilt(sos, acc_x_list)
  filtered = filtered.tolist()

  ######### for resampling of data  ###############################
  df = resample_acc_x(filtered)
  acc_x_list = df.acc_x.tolist()
  #################################################################

  return df, acc_x_list

"""#Not Overlapping Window of size 1024"""

def per_ent_nov(acc_x_list):
  permutation_entropy_list = []
  i = 0
  j = 1024
  time_count = 0
  while(j<len(acc_x_list)):
    t_start = time.time()
    val = ordpy.permutation_entropy(acc_x_list[i:j], dx = 6)
    t_end = time.time()
    time_count+=(t_end - t_start)
    permutation_entropy_list.append(val)
    i = j
    j+= 1024
  time_elapsed = time_count/len(permutation_entropy_list)
  print("The time taken to calculate the value of permutation entropy for a window of size 1024 samples within a time-span of 100 microseconds is {} seconds".format(time_elapsed))
  return permutation_entropy_list

"""# time to frequency conversion"""

def time_2_freq_conversion(df):
    Fx = np.array(df.acc_x)
    X = scipy.fft.fft(Fx)
    N = len(X)
    n = np.arange(N)
    sr = 1 / (10000)
    T = N/sr
    freq = n/T 

    # Get the one-sided specturm
    n_oneside = N//2
    # get the one side frequency
    f_oneside = freq[:n_oneside]

    amplitude = np.abs(X[:n_oneside])
    freq = f_oneside

    return freq, amplitude

"""# Power Spectral Density


"""

def psd(acc_x_list):
  input_length = len(acc_x_list)
  f_w_mean, ps_w_mean = signal.welch(acc_x_list, fs = 10000, nfft = 1024)
  f_w_median, ps_w_median = signal.welch(acc_x_list, fs = 10000, average = "median", nfft = 1024)
  freq, time, spectrogram = signal.spectrogram(np.array(acc_x_list), fs = 10000, nperseg = 1024)

  fig, ax = plt.subplots(4, 2, figsize = (20,20), constrained_layout = True)

  mat_nor_psd, mat_nor_freq, mat_nor_line = ax[0,0].psd(acc_x_list, NFFT = 1024, Fs = 10000, return_line = True)
  ax[0,0].set_title('Matplotlib_psd_using_welch_avg_periodogram')

  mat_magnitude_spectrum, mat_magnitude_spectrum_freq, mat_magnitude_spectrum_line = ax[0,1].magnitude_spectrum(acc_x_list, Fs = 10000)
  ax[0,1].set_title('Matplotlib_magnitude_spectrum')

  sci_welch_mean_line = ax[1,0].semilogy(f_w_mean, ps_w_mean, label = "mean")
  sci_welch_median_line = ax[1,0].semilogy(f_w_median, ps_w_median, label = "median")
  ax[1,0].set_title('Scipy_psd_using_welch')
  ax[1,0].set_xlabel('frequency [Hz]')
  ax[1,0].set_ylabel('PSD [V**2/Hz]')
  ax[1,0].legend(loc = "upper right")

  mat_log_magnitude_spectrum, mat_log_magnitude_spectrum_freq, mat_log_magnitude_spectrum_line = ax[1,1].magnitude_spectrum(acc_x_list, Fs = 10000, scale = "dB")
  ax[1,1].set_title('Matplotlib_log_magnitude_spectrum')

  mat_angle_spectrum, mat_angle_spectrum_freq, mat_angle_spectrum_line = ax[2,0].angle_spectrum(acc_x_list, Fs = 10000)
  ax[2,0].set_title("Angle_spectrum")

  mat_phase_spectrum, mat_phase_spectrum_freq, mat_phase_spectrum_line = ax[2,1].phase_spectrum(acc_x_list, Fs = 10000)
  ax[2,1].set_title("Phase_spectrum")

  spectrum, fre, tim, im = ax[3,0].specgram(acc_x_list, Fs = 10000, NFFT = 1024)
  ax[3,0].set_title("Matplotlib_spectrogram")
  ax[3,0].set_ylabel('Frequency [Hz]')
  ax[3,0].set_xlabel('Time [sec]')
  
  ax[3,1].pcolormesh(time, freq, spectrogram, cmap = "YlOrRd")
  ax[3,1].set_title('Scipy_spectrogram')
  ax[3,1].set_ylabel('Frequency [Hz]')
  ax[3,1].set_xlabel('Time [sec]')
  #fig.tight_layout

  return_list = [mat_nor_psd, mat_nor_freq, mat_nor_line, mat_magnitude_spectrum, mat_magnitude_spectrum_freq, mat_magnitude_spectrum_line,
  f_w_mean, ps_w_mean, sci_welch_mean_line, f_w_median, ps_w_median, sci_welch_median_line,
  mat_log_magnitude_spectrum, mat_log_magnitude_spectrum_freq, mat_log_magnitude_spectrum_line,
  mat_angle_spectrum, mat_angle_spectrum_freq, mat_angle_spectrum_line,
  mat_phase_spectrum, mat_phase_spectrum_freq, mat_phase_spectrum_line,
  spectrum, fre, tim, im, freq, time, spectrogram]

  return return_list

def feature_func(path):
  df, acc_x_list = data_func(path)
  # permutation_entropy_list = per_ent_nov(acc_x_list)
  # freq, amplitude = time_2_freq_conversion(df)
  # parameter_list = psd(acc_x_list)
  # return df, acc_x_list, permutation_entropy_list, freq, amplitude, parameter_list


  ##########################################################################################
  return acc_x_list
  ##########################################################################################

path_list = []

############## 2 #################

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p005.mat"] ## 0/2/320/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p010.mat"] ## 1/2/320/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p015.mat"] ## 2/2/320/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p020.mat"] ## 3/2/320/20
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p025.mat"] ## 4/2/320/25
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p030.mat"] ## 5/2/320/30
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p035.mat"] ## 6/2/320/35
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p040.mat"] ## 7/2/320/40
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p045.mat"] ## 8/2/320/45
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm320_doc0p050.mat"] ## 9/2/320/50

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm425_doc0p005.mat"] ## 10/2/425/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm425_doc0p010.mat"] ## 11/2/425/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm425_doc0p015.mat"] ## 12/2/425/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm425_doc0p017.mat"] ## 13/2/425/17
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm425_doc0p020.mat"] ## 14/2/425/20
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm425_doc0p025.mat"] ## 15/2/425/25

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm570_doc0p001.mat"] ## 16/2/570/1
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm570_doc0p002.mat"] ## 17/2/570/2
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm570_doc0p005.mat"] ## 18/2/570/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm570_doc0p010.mat"] ## 19/2/570/10

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm770_doc0p001.mat"] ## 20/2/770/1
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm770_doc0p002.mat"] ## 21/2/770/2
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm770_doc0p005.mat"] ## 22/2/770/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm770_doc0p010.mat"] ## 23/2/770/10

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2inchStickout/F_08-Jun-2017_rpm1030_doc0p001.mat"] ## 24/2/1030/1


################### 2.5 #################

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p003.mat"] ## 25/2.5/570/3
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p005.mat"] ## 26/2.5/570/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p008.mat"] ## 27/2.5/570/8
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p010.mat"] ## 28/2.5/570/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p012.mat"] ## 29/2.5/570/12
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p014.mat"] ## 30/2.5/570/14
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p015.mat"] ## 31/2.5/570/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm570_doc0p015_trial2.mat"] ## 32/2.5/570/15/trial2

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm770_doc0p002.mat"] ## 33/2.5/770/2
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_2p5inchStickout/F_12-Jun-2017_rpm770_doc0p005.mat"] ## 34/2.5/770/5


############## 3.5 #####################

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_12-Jun-2017_rpm570_doc0p015.mat"] ## 35/3.5/570/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_12-Jun-2017_rpm570_doc0p025.mat"] ## 36/3.5/570/25
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_12-Jun-2017_rpm570_doc0p030.mat"] ## 37/3.5/570/30

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_14-Jun-2017_rpm770_doc0p005.mat"] ## 38/3.5/770/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_14-Jun-2017_rpm770_doc0p008.mat"] ## 39/3.5/770/8
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_14-Jun-2017_rpm770_doc0p010.mat"] ## 40/3.5/770/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_14-Jun-2017_rpm770_doc0p015.mat"] ## 41/3.5/770/15

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_3p5inchStickout/F_14-Jun-2017_rpm1030_doc0p002.mat"] ## 42/3.5/1030/2


################ 4.5 ####################

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_12-Jun-2017_rpm570_doc0p005.mat"] ## 43/4.5/570/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_12-Jun-2017_rpm570_doc0p010.mat"] ## 44/4.5/570/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_12-Jun-2017_rpm570_doc0p015.mat"] ## 45/4.5/570/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_12-Jun-2017_rpm570_doc0p025.mat"] ## 46/4.5/570/25
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_12-Jun-2017_rpm570_doc0p035.mat"] ## 47/4.5/570/35
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_12-Jun-2017_rpm570_doc0p040.mat"] ## 48/4.5/570/40

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm770_doc0p010.mat"] ## 49/4.5/770/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm770_doc0p015.mat"] ## 50/4.5/770/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm770_doc0p020.mat"] ## 51/4.5/770/20

path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p005.mat"] ## 52/4.5/1030/5
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p007.mat"] ## 53/4.5/1030/7
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p010.mat"] ## 54/4.5/1030/10
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p012.mat"] ## 55/4.5/1030/12
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p013.mat"] ## 56/4.5/1030/13
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p014.mat"] ## 57/4.5/1030/14
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p015.mat"] ## 58/4.5/1030/15
path_list += ["drive/MyDrive/cutting_tests_raw/timeSeries_4p5inchStickout/F_13-Jun-2017_rpm1030_doc0p016.mat"] ## 59/4.5/1030/16

# index_list = [2, 12, 19, 24, 28, 32, 34, 37, 40, 42, 47, 51, 54, 58]
# ac_mean_list = []
# ac_median_list = []
# ac_median_low_list = []
# ac_median_high_list = []
# ac_median_grouped_list = []
# ac_stdev = []
# ac_variance = []
# pe_mean_list = []
# pe_median_list = []
# pe_median_low_list = []
# pe_median_high_list = []
# pe_median_grouped_list = []
# pe_stdev = []
# pe_variance = []
##############################
# df_list = []

ac_list_list = []

# pe_list_list = []
# freq_list = []
# amplitude_list = []
# para = []
##############################
start_time_1 = time.time()
for i in range(len(path_list)):
  # df, ac_list, pe_list, freq, amplitude, parameter_list = feature_func(path_list[i])

  ################################################################################
  ac_list = feature_func(path_list[i])
  ac_list_list.append(ac_list)
  ###############################################################################
end_time_1 = time.time()
print(f"filtering and resampling time of total time series (15500 samples): {end_time_1 - start_time_1}")
print(f"filtering and resampling time of 1000 smaples for real-time analysis: {(end_time_1 - start_time_1)/len(ac_list_list)}")
  # df_list.append(df)
  # pe_list_list.append(pe_list)
  # freq_list.append(freq)
  # amplitude_list.append(amplitude)
  # para.append(parameter_list)

#######################################################################

  # ac_mean_list.append(statistics.mean(ac_list))
  # ac_median_list.append(statistics.median(ac_list))
  # ac_median_low_list.append(statistics.median_low(ac_list))
  # ac_median_high_list.append(statistics.median_high(ac_list))
  # ac_median_grouped_list.append(statistics.median_grouped(ac_list))
  # ac_stdev.append(statistics.stdev(ac_list))
  # ac_variance.append(statistics.variance(ac_list))

  # pe_mean_list.append(statistics.mean(pe_list))
  # pe_median_list.append(statistics.median(pe_list))
  # pe_median_low_list.append(statistics.median_low(pe_list))
  # pe_median_high_list.append(statistics.median_high(pe_list))
  # pe_median_grouped_list.append(statistics.median_grouped(pe_list))
  # pe_stdev.append(statistics.stdev(pe_list))
  # pe_variance.append(statistics.variance(pe_list))

# df_res = pd.DataFrame(
#     {
#         'ac_mean' : ac_mean_list,
#         'ac_median': ac_median_list,
#         'ac_median_low' : ac_median_low_list,
#         'ac_median_high' : ac_median_high_list,
#         'ac_median_grouped' : ac_median_grouped_list,
#         'ac_std_dev': ac_stdev,
#         'ac_variance' : ac_variance,
      
#         'pe_mean' : pe_mean_list,
#         'pe_median' : pe_median_list,
#         'pe_median_low' : pe_median_low_list,
#         'pe_median_high' : pe_median_high_list,
#         'pe_median_grouped' : pe_median_grouped_list,
#         'pe_std_dev' : pe_stdev,
#         'pe_variance' : pe_variance
#     }
# )
#

"""# Plot 1 peak list"""

# mat_psd_peak_list = []
# for i in range(len(para)):
#   l = para[i][2][0].get_xydata()
#   n = len(l)
#   max_ind = 19
#   for j in range(20, n):
#     if ((l[max_ind][1]<l[j][1]) and (l[j][0]<=3000)):
#       max_ind = j
#   mat_psd_peak_list.append(math.exp(l[max_ind][1]))

# mat_psd_peak_list

"""# Plot 1 threshold count list """

# mat_psd_count_threshold_list = []
# for i in range(len(para)):
#   l = para[i][2][0].get_xydata()
#   count = 0
#   for j in range(len(l)):
#     if ((l[j][1])>-75) and (l[j][0]<=3000):
#       count += 1
#   mat_psd_count_threshold_list.append(count)

# mat_psd_count_threshold_list

"""# Plot 2 Area under the curve list"""

# mat_mag_spec_auc_list = []
# for i in range(len(para)):
#   l = para[i][5].get_xdata()
#   ind = -1
#   for j in range(len(l)):
#     if(l[j]>3000):
#       ind = j
#       break;
#   x_data = l[5:ind]
#   y_data = para[i][5].get_ydata()[5:ind]
#   mat_mag_spec_auc_list.append(math.exp(sklearn.metrics.auc(x_data, y_data) * 1000))

# mat_mag_spec_auc_list

"""# Plot 2 threshold count list"""

# mat_mg_spec_count_threshold_list = []
# for i in range(len(para)):
#   l = para[i][5].get_xydata()
#   count = 0
#   for j in range(len(l)):
#     if ((l[j][1])>0.00008) and (l[j][0]<=3000):
#       count += 1
#   mat_mg_spec_count_threshold_list.append(count)

# mat_mg_spec_count_threshold_list

"""# Plot 3 threshold count list for median value"""

# sci_median_count_threshold_list = []
# for i in range(len(para)):
#   l = para[i][11][0].get_xydata()
#   count = 0
#   for j in range(len(l)):
#     if ((l[j][1])>(pow(10,-8))) and (l[j][0]<=3000):
#       count += 1
#   sci_median_count_threshold_list.append(count)

# sci_median_count_threshold_list

"""# Plot 3 max median value"""

# sci_psd_median_peak_list = []
# for i in range(len(para)):
#   l = para[i][11][0].get_xydata()
#   n = len(l)
#   max_ind = 19
#   for j in range(20, n):
#     if ((l[max_ind][1]<l[j][1]) and (l[j][0]<=3000)):
#       max_ind = j
#   sci_psd_median_peak_list.append((((math.exp(l[max_ind][1])) * pow(10,16)) % pow(10,16))/ 1000000)

# sci_psd_median_peak_list

"""# Plot 3 threshold count list for mean value"""

# sci_mean_count_threshold_list = []
# for i in range(len(para)):
#   l = para[i][8][0].get_xydata()
#   count = 0
#   for j in range(len(l)):
#     if ((l[j][1])>(pow(10,-8))) and (l[j][0]<=3000):
#       count += 1
#   sci_mean_count_threshold_list.append(count)

# sci_mean_count_threshold_list

"""# Plot 3 max mean value"""

# sci_psd_mean_peak_list = []
# for i in range(len(para)):
#   l = para[i][8][0].get_xydata()
#   n = len(l)
#   max_ind = 19
#   for j in range(20, n):
#     if ((l[max_ind][1]<l[j][1]) and (l[j][0]<=3000)):
#       max_ind = j
#   sci_psd_mean_peak_list.append((((math.exp(l[max_ind][1])) * pow(10,16)) % pow(10,16))/ 1000000)

# sci_psd_mean_peak_list

# sci_spec_threshold_mean_list = []
# for k in range(len(para)):
#   sp = para[k][27]
#   fr = para[k][25]
#   ind = -1
#   for i in range(len(fr)):
#     if (fr[i]>3000):
#       ind = i
#       break
#   tim_list = []
#   for i in range(ind):
#     ti = statistics.median(sp[i])
#     tim_list.append(ti)
#   max_ind = np.argmax(tim_list)
#   sci_spec_threshold_mean_list.append(tim_list[max_ind])

# sci_spec_threshold_mean_list

len(ac_list_list)

# Retrieves a pre-defined feature configuration file to extract all available features
cfg = tsfel.get_features_by_domain()

# Extract features
X_list = []
start_time = time.time()
for i in range(len(ac_list_list)):
  X = tsfel.time_series_features_extractor(cfg, ac_list_list[i], fs = 10000, window_size = 1000)
  X_list.append(X)
end_time = time.time()
print(f"feature extraction time of all the time series (15500 samples): {end_time - start_time}")
print(f"feature_extraction_time of 1000 samples for real-time analysis: {(end_time - start_time)/len(ac_list_list)}")
df_ans = pd.concat(X_list, ignore_index = True)

df_ans

min = 100000000000000
for i in range(len(ac_list_list)):
  n = len(ac_list_list[i])
  if (min>n):
    min = n
print(min)

filtering_resampling_time_1000_samples = 13.207/min
feature_extraction_time_1000_samples = 19.182/min
print(f"filtering resampling time 1000 samples for real-time analysis: {filtering_resampling_time_1000_samples} seconds")
print(f"feature extraction time 1000 samples for real-time analysis: {feature_extraction_time_1000_samples} seconds")

# Extract features
X_list = []
for i in range(len(ac_list_list)):
  X = tsfel.time_series_features_extractor(cfg, ac_list_list[i], fs = 10000)
  X_list.append(X)
df_ans_1 = pd.concat(X_list, ignore_index = True)

df_ans_1

label_classification_list = ["chatter"]
label_classification_list += ["no_chatter"] * 4
label_classification_list += ["mild_chatter"]
label_classification_list += ["no_chatter"] * 8
label_classification_list += ["chatter"] * 11
label_classification_list += ["no_chatter"]
label_classification_list += ["chatter"]
label_classification_list += ["no_chatter"] * 2
label_classification_list += ["mild_chatter"] * 2
label_classification_list += ["chatter"] * 3
label_classification_list += ["mild_chatter"] 
label_classification_list += ["no_chatter"] * 5
label_classification_list += ["mild_chatter"] 
label_classification_list += ["chatter"] * 2
label_classification_list += ["no_chatter"] * 5
label_classification_list += ["chatter"]
label_classification_list += ["no_chatter"] * 5
label_classification_list += ["mild_chatter"] * 4
label_classification_list += ["chatter"]
label_classification_list += ["mild_chatter"]

stickout_list = [2] * 25
stickout_list += [2.5] * 10
stickout_list += [3.5] * 8
stickout_list += [4.5] * 17

cs_list = [320] * 10
cs_list += [425] * 6
cs_list += [570] * 4
cs_list += [770] * 4
cs_list += [1030] * 1
cs_list += [570] * 8
cs_list += [770] * 2
cs_list += [570] * 3
cs_list += [770] * 4
cs_list += [1030] * 1
cs_list += [570] * 6
cs_list += [770] * 3
cs_list += [1030] * 8

doc_list = [5,10,15,20,25,30,35,40,45,50, 5,10,15,17,20,25, 1,2,5,10, 1,2,5,10, 1,3,5,8,10,12,14,15,15, 2,5,15,25,30, 5,8,10,15, 2, 5,10,15,25,35,40, 10,15,20, 5,7,10,12,13,14,15,16]

df_ans_1.insert(0, "stickout_distance_inches", stickout_list)
df_ans_1.insert(1, "cutting_speed_rpm", cs_list)

df_ans_1.insert(2, "depth_of_cut_milliinches", doc_list)
df_ans_1["category"] = label_classification_list

df_ans_1

# df_ans_1.to_csv("feature_data.csv", index = False, encoding = "utf-8")

df_ans

# df_ans.to_csv("feature_data_window.csv", index = False, encoding = "utf-8")

data_list = [[2,320,5], [2,320,10], [2,320,15], [2,320,20], [2,320,25], [2,320,30],
             [2,320,35],[2,320,40],[2,320,45],[2,320,50],[2,425,5],[2,425,10],
             [2,425,15],[2,425,17],[2,425,20],[2,425,25],[2,570,1],[2,570,2],
             [2,570,5],[2,570,10],[2,770,1],[2,770,2],[2,770,5],[2,770,10],
             [2,1030,1],[2.5,570,3],[2.5,570,5],[2.5,570,8],[2.5,570,10],[2.5,570,12],
             [2.5,570,14],[2.5,570,15],[2.5,570,15],[2.5,770,2],[2.5,770,5],[3.5,570,15],
             [3.5,570,25],[3.5,570,30],[3.5,770,5],[3.5,770,8],[3.5,770,10],[3.5,770,15],
             [3.5,1030,2],[4.5,570,5],[4.5,570,10],[4.5,570,15],[4.5,570,25],[4.5,570,35],
             [4.5,570,40],[4.5,770,10],[4.5,770,15],[4.5,770,20],[4.5,1030,5],[4.5,1030,7],
             [4.5,1030,10],[4.5,1030,12],[4.5,1030,13],[4.5,1030,14],[4.5,1030,15],[4.5,1030,16]]

lis = []
for i in range(len(ac_list_list)):
  n = len(ac_list_list[i])//1000
  for j in range(n):
    lis.append(data_list[i])

df_new = pd.DataFrame(lis, columns = ["stickout_distance_inches","cutting_speed_rpm","depth_of_cut_milliinches"])

df_new_1 = pd.concat([df_new, df_ans], axis = 1)

df_new_1

# df_new_1.to_csv("feature_data_window_1.csv", index = False, encoding = "utf-8")

chatter_time = [
                [(13.5, 32.49)],[(-1, -1)],[(-1,-1)],[(-1, -1)],[(-1,-1)],[(46, 60)],
                [(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],
                [(-1, -1)],[(11.5, 12)],[(7.5, 12.5), (13.5, 16), (16, 20)],[(8.5, 20)],[(4.75, 14.5)],[(4.8, 9.9)],
                [(5.3, 7.8)],[(4.4, 14.5)],[(16, 20)],[(4, 7.3), (16, 18.5)],[(2.49, 24.99)],[(4.99, 29.99)],
                [(2.2, 10)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(3.5, 5)],[(4, 15)],
                [(5, 7.5)],[(5.2, 6), (24, 25)],[(5, 14.5)],[(8, 9.5)],[(3.5, 4.4), (8, 12)],[(-1, -1)],
                [(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(6.2, 8.6),(9.2, 11.5)],
                [(3.3, 8)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],
                [(15, 30)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],[(-1, -1)],
                [(3, 5), (6.2, 9)],[(3.8, 10)],[(3.75, 6)],[(3, 6)],[(3.6, 10)],[(3.2, 10)]

]

time_label_list = []
for i in range(len(ac_list_list)):
  n = len(ac_list_list[i])//1000
  l = chatter_time[i]
  start = 0
  end = 0.1
  current = 0
  for j in range(n):
    if (l[current][0] != -1 and l[current][1] != -1):
      if(end <= l[current][0]):
        time_label_list.append(l[current][0] - end)
      elif (end > l[current][0] and start < l[current][1]):
        time_label_list.append(0)
      elif (start >= l[current][1] and current + 1 != len(l)):
        current += 1
        time_label_list.append(l[current][0] - end)
      elif (start >= l[current][1] and current + 1 == len(l)):
        time_label_list.append(-1)
      start = end
      end += 0.1 
    else:
      time_label_list.append(-1)

df_new_1["time_of_onset_of_chatter_in_sec"] = time_label_list

df_new_1

df_new_1.to_csv("feature_data_regression.csv", index = False, encoding = "utf-8")

