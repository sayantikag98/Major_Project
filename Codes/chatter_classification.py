# -*- coding: utf-8 -*-
"""Chatter_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gV6OSA5oOJGWXoXrcfA-_e0N7xaxG7n9

# MOUNTING GOOGLE DRIVE
"""

from google.colab import drive 
drive.mount('drive', force_remount = False)

"""# IMPORTING PYTHON LIBRARIES"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import scipy
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
import statsmodels.api as sm
from sklearn.feature_selection import chi2
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedKFold
import warnings
warnings.simplefilter("ignore")
from sklearn.metrics import classification_report

"""# DATA LOADING"""

path = "drive/MyDrive/feature_data_classification.csv"
df = pd.read_csv(path)

df

"""# DATA EXPLORATION"""

df.describe()

df.isna().sum()

"""# FEATURE SCALING - NORMALIZATION"""

minmax = MinMaxScaler(feature_range = (0, 1))
for x in df.columns:
  if (x != "stickout_distance_inches" and x != "cutting_speed_rpm" and x != "category"):
    df[x]  = minmax.fit_transform(np.array(df[x]).reshape(-1,1))

df

"""# LABELS CONVERTED TO INTERGERS FROM STRINGS"""

df["category"].replace({"no_chatter" : 0, "mild_chatter" : 1, "chatter" : 2}, inplace = True)

df.shape

"""# DROPPING COLUMNS HAVING SAME VALUE"""

df.drop(df.std()[(df.std() == 0)].index, axis=1, inplace = True)

df

"""# LINE PLOT"""

# df.plot(subplots = True, linewidth = 2.0, figsize = (30, 500))
# plt.legend(loc = "upper right")
# plt.show()

"""# HEAT MAP"""

# plt.figure(figsize = (400, 600))
# corr = df.iloc[:,3:-1].corr()
# sns.heatmap(corr, annot = True, cmap = "tab20")
# plt.show()

"""# ONE HOT ENCODING"""

sd = pd.get_dummies(df['stickout_distance_inches'], prefix = "stickout")

cs = pd.get_dummies(df.cutting_speed_rpm, prefix = "cutting_speed")

df = pd.concat([sd,cs, df.iloc[:,2:]], axis = 1)

"""# FEATURE SCALING USING VARIANCE INFLATION FACTOR (VIF)"""

def calc_vif(X):
    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return(vif)

vif_res = calc_vif(df.iloc[:,10:-1])

col_name = vif_res[vif_res.VIF>10].iloc[:,0].tolist()

df.shape

"""# FEATURES DROPPED FOR HIGH VIF"""

# df.drop(columns = col_name, inplace = True, axis = 1)

df.shape

"""# FEATURE LABEL SPLIT"""

X = df.iloc[:,:-1].values

X.shape

y = df.iloc[:,-1].values

y.shape

"""# TRAIN VALIDATION TEST SPLIT"""

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 101)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size = 0.8, random_state = 101)

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)
print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

"""# CLASSIFICATION MODELS"""

def classifier(X, y):
  random_state = 12883823
  rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)
  for train, test in rkf.split(X,y):
    X_train1=X[train]
    X_test1=X[test]
    y_train1=y[train]
    y_test1=y[test]
  models = {
          'Random Forest': RandomForestClassifier(),
          'AdaBoost': AdaBoostClassifier(),
          'Gradient Boosting': GradientBoostingClassifier(),
          'SVM': SVC(probability=True),
          'Decision Tree': DecisionTreeClassifier()
      }

  params = {
          'Random Forest': { 'n_estimators': [16, 32] },
          'AdaBoost':  { 'n_estimators': [16, 32] },
          'Gradient Boosting': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },
          'SVM': {'kernel': ['rbf','linear'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},
          'Decision Tree': {'criterion':['gini','entropy']}
                
      }
  result=[]
  keys = models.keys()
  for key in keys:
    model = models[key]
    param = params[key]
  ##### for hyperparameter tuning by using GridSearchCV ###
    grid = GridSearchCV(model, param)
    grid.fit(X_train1,y_train1)
    prediction=grid.predict(X_test1)
    scoring = ['precision_macro', 'recall_macro','f1_macro']
    scores = cross_val_score(grid, X, y, cv=5, scoring='f1_macro')
    scores1= cross_validate(grid, X, y, scoring=scoring,
                          cv=5, return_train_score=False)
    result.append(
              {
                  'classifier_name': key,
                  # 'grid': grid,
                  # 'classifier': grid.best_estimator_,
                  'best score': grid.best_score_,
                  # 'best params': grid.best_params_,
                  # 'cv': grid.cv,
                  'prediction': prediction,
                  'cv_score': scores.mean(),
                  'cv_validate_score': scores1 ['test_f1_macro'],
                  'probability': grid.predict_proba(X_test1),
                  # 'accuracy_score': accuracy_score(y_test1,prediction),
                  # 'average_precision_score': metrics.average_precision_score(y_test1,prediction, average = 'micro'),
                  'balanced_accuracy_score': metrics.balanced_accuracy_score(y_test1,prediction),
                  'classification report' : metrics.classification_report(y_test1,prediction, target_names = ['no_chatter', 'mild_chatter', 'chatter']),
                  'cohen kappa score' : metrics.cohen_kappa_score(y_test1,prediction),
                  'confusion matrix' : metrics.confusion_matrix(y_test1,prediction),
                  'precision_score' : metrics.precision_score(y_test1,prediction, average = 'macro'),
                  'recall_score' : metrics.recall_score(y_test1,prediction, average = 'macro'),
                  'f1 score': metrics.f1_score(y_test1,prediction, average = 'macro'),
                  'Area Under the Receiver Operating Characteristic Curve': metrics.roc_auc_score(y_test1,grid.predict_proba(X_test1), multi_class = "ovr", average = 'macro'),
                  # 'Receiver operating characteristic (ROC)': metrics.roc_curve(y_test1,prediction),
                  'Precision-recall-f1 score for each class': metrics.precision_recall_fscore_support(y_test1,prediction, average = 'macro')

              }
          )

  # print(result)
  d=pd.DataFrame(result)
  ### sort on the basis of cross-validation score scoring method being f1
  d.sort_values(by='f1 score',inplace=True, ascending = False)
  return d

d = classifier(X,y)

d

d.to_csv("result_classification.csv", index = False, encoding = "utf-8")

